{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "\n",
    "\n",
    "class CVAE(pl.LightningModule):\n",
    "    def __init__(self, input_dim, latent_dim, output_dim, lr=1e-3):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim + output_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, latent_dim * 2)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim + output_dim, 256)\n",
    "        self.fc4 = nn.Linear(256, input_dim)\n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        input_combined = torch.cat((x, y), dim=1)\n",
    "        hidden = F.relu(self.fc1(input_combined))\n",
    "        latent_params = self.fc2(hidden)\n",
    "        return latent_params\n",
    "    \n",
    "    def decode(self, z, y):\n",
    "        latent_combined = torch.cat((z, y), dim=1)\n",
    "        hidden = F.relu(self.fc3(latent_combined))\n",
    "        output = self.fc4(hidden)\n",
    "        return output\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        latent_params = self.encode(x, y)\n",
    "        mu, logvar = latent_params[:, :self.latent_dim], latent_params[:, self.latent_dim:]\n",
    "        \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z, y)\n",
    "        return reconstructed, mu, logvar\n",
    "    \n",
    "    def loss_function(self, reconstructed, x, mu, logvar):\n",
    "        reconstruction_loss = F.mse_loss(reconstructed, x, reduction='sum')\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return reconstruction_loss + kl_divergence\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        reconstructed, mu, logvar = self(x, y)\n",
    "        loss = self.loss_function(reconstructed, x, mu, logvar)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        reconstructed, mu, logvar = self(x, y)\n",
    "        loss = self.loss_function(reconstructed, x, mu, logvar)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.loggers.logger import rank_zero_experiment\n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "class DictLogger(Logger):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        def def_value(): \n",
    "            return []\n",
    "              \n",
    "        # Defining the dict \n",
    "        self.metrics = defaultdict(def_value) \n",
    "\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'DictLogger'\n",
    "\n",
    "    @property\n",
    "    @rank_zero_experiment\n",
    "    def experiment(self):\n",
    "        # Return the experiment object associated with this logger.\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        # Return the experiment version, int or str.\n",
    "        return '0.1'\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        # params is an argparse.Namespace\n",
    "        # your code to record hyperparameters goes here\n",
    "        pass\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step):\n",
    "        # metrics is a dictionary of metric names and values\n",
    "        # your code to record metrics goes here\n",
    "        for key in metrics.keys():\n",
    "            self.metrics[key].append(metrics[key])\n",
    "\n",
    "    @rank_zero_only\n",
    "    def save(self):\n",
    "        # Optional. Any code necessary to save logger data goes here\n",
    "        # If you implement this, remember to call `super().save()`\n",
    "        # at the start of the method (important for aggregation of metrics)\n",
    "        super().save()\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        # Optional. Any code that needs to be run after training\n",
    "        # finishes goes here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, batch_size=64):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # Split the dataset into train and validation sets\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        val_size = len(self.dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(self.dataset, [train_size, val_size])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/ssilvari/Downloads/fedcombat_real_data/unified/non_harmonized_data.csv', low_memory=False, index_col=0).dropna(how='any')\n",
    "\n",
    "formula = 'site ~ '\n",
    "for i, col in enumerate(df.columns):\n",
    "    if col in ['site']:\n",
    "        continue\n",
    "    if df[col].dtype.kind in 'biufc':\n",
    "        formula += f' standardize(Q(\"{col}\")) '\n",
    "    else:\n",
    "        formula += f' C(Q(\"{col}\")) '\n",
    "\n",
    "    if i != len(df.columns) - 1:\n",
    "        formula += '+'\n",
    "formula += ' -1'\n",
    "\n",
    "Y, X = patsy.dmatrices(formula, df, return_type='matrix')\n",
    "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(Y).float())\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = Y.shape[1]\n",
    "latent_dim = 31\n",
    "\n",
    "cvae = CVAE(input_dim, latent_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# data_module = MyDataModule(dataset)\n",
    "\n",
    "# logger = DictLogger()\n",
    "# logger = TensorBoardLogger(save_dir='tensorboard_logs', name='MECVAE')\n",
    "\n",
    "# early_stop_callback = EarlyStopping('val_loss')\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs=100, logger=logger, callbacks=[early_stop_callback])\n",
    "# trainer.fit(cvae, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    latent_dim = trial.suggest_int('latent_dim', 200, 300)  # Adjust the range as per your requirements\n",
    "    # latent_dim = 212\n",
    "    batch_size = trial.suggest_int('batch_size', 3, 64)\n",
    "    # lr = trial.suggest_loguniform('lr', 1e-5, 0.1)\n",
    "    lr = 2.964865522829201e-05\n",
    "    # lr = 1e-1\n",
    "    \n",
    "    # Create the CVAE model with the suggested latent_dim\n",
    "    cvae = CVAE(input_dim, latent_dim, output_dim, lr=lr)\n",
    "    \n",
    "    # Create the LightningDataModule with the desired train and validation datasets\n",
    "    data_module = MyDataModule(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Create the Trainer\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(cvae, data_module)\n",
    "    \n",
    "    # Return the validation loss as the objective value for Optuna to optimize\n",
    "    return trainer.callback_metrics['val_loss'].item()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the best number of latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:09:20,942]\u001b[0m A new study created in RDB with name: no-name-a8f0af81-c0e1-4c27-8be8-b9e11ae303ea\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 151 K \n",
      "2 | fc3  | Linear | 77.8 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "280 K     Trainable params\n",
      "0         Non-trainable params\n",
      "280 K     Total params\n",
      "1.123     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 129/129 [00:02<00:00, 52.28it/s, v_num=161, train_loss=1.35e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 129/129 [00:02<00:00, 51.96it/s, v_num=161, train_loss=1.35e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:09:53,111]\u001b[0m Trial 0 finished with value: 2524.949462890625 and parameters: {'latent_dim': 294, 'batch_size': 45}. Best is trial 0 with value: 2524.949462890625.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 131 K \n",
      "2 | fc3  | Linear | 67.8 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "250 K     Trainable params\n",
      "0         Non-trainable params\n",
      "250 K     Total params\n",
      "1.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 92/92 [00:01<00:00, 59.34it/s, v_num=162, train_loss=3.11e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 92/92 [00:01<00:00, 58.83it/s, v_num=162, train_loss=3.11e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:10:14,537]\u001b[0m Trial 1 finished with value: 3605.8359375 and parameters: {'latent_dim': 255, 'batch_size': 63}. Best is trial 0 with value: 2524.949462890625.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 104 K \n",
      "2 | fc3  | Linear | 54.5 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "210 K     Trainable params\n",
      "0         Non-trainable params\n",
      "210 K     Total params\n",
      "0.843     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 102/102 [00:01<00:00, 52.93it/s, v_num=163, train_loss=1.37e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 102/102 [00:01<00:00, 52.60it/s, v_num=163, train_loss=1.37e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:10:36,071]\u001b[0m Trial 2 finished with value: 3189.458251953125 and parameters: {'latent_dim': 203, 'batch_size': 57}. Best is trial 0 with value: 2524.949462890625.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 130 K \n",
      "2 | fc3  | Linear | 67.6 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "249 K     Trainable params\n",
      "0         Non-trainable params\n",
      "249 K     Total params\n",
      "1.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 104/104 [00:01<00:00, 54.94it/s, v_num=164, train_loss=949.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 104/104 [00:01<00:00, 54.55it/s, v_num=164, train_loss=949.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:10:55,444]\u001b[0m Trial 3 finished with value: 3159.2734375 and parameters: {'latent_dim': 254, 'batch_size': 56}. Best is trial 0 with value: 2524.949462890625.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 145 K \n",
      "2 | fc3  | Linear | 75.3 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "273 K     Trainable params\n",
      "0         Non-trainable params\n",
      "273 K     Total params\n",
      "1.092     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 145/145 [00:02<00:00, 59.04it/s, v_num=165, train_loss=1.19e+3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 145/145 [00:02<00:00, 58.71it/s, v_num=165, train_loss=1.19e+3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:11:23,687]\u001b[0m Trial 4 finished with value: 2225.6318359375 and parameters: {'latent_dim': 284, 'batch_size': 40}. Best is trial 4 with value: 2225.6318359375.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 115 K \n",
      "2 | fc3  | Linear | 59.9 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "226 K     Trainable params\n",
      "0         Non-trainable params\n",
      "226 K     Total params\n",
      "0.907     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 171/171 [00:03<00:00, 55.53it/s, v_num=166, train_loss=261.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 171/171 [00:03<00:00, 55.31it/s, v_num=166, train_loss=261.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:11:55,346]\u001b[0m Trial 5 finished with value: 1844.955078125 and parameters: {'latent_dim': 224, 'batch_size': 34}. Best is trial 5 with value: 1844.955078125.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 106 K \n",
      "2 | fc3  | Linear | 55.6 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "213 K     Trainable params\n",
      "0         Non-trainable params\n",
      "213 K     Total params\n",
      "0.855     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:02<00:00, 59.72it/s, v_num=167, train_loss=647.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 157/157 [00:02<00:00, 59.46it/s, v_num=167, train_loss=647.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:12:26,019]\u001b[0m Trial 6 finished with value: 2054.93115234375 and parameters: {'latent_dim': 207, 'batch_size': 37}. Best is trial 5 with value: 1844.955078125.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 145 K \n",
      "2 | fc3  | Linear | 75.3 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "273 K     Trainable params\n",
      "0         Non-trainable params\n",
      "273 K     Total params\n",
      "1.092     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 124/124 [00:02<00:00, 54.57it/s, v_num=168, train_loss=191.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 124/124 [00:02<00:00, 54.12it/s, v_num=168, train_loss=191.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:12:49,394]\u001b[0m Trial 7 finished with value: 2620.73681640625 and parameters: {'latent_dim': 284, 'batch_size': 47}. Best is trial 5 with value: 1844.955078125.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 116 K \n",
      "2 | fc3  | Linear | 60.4 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "228 K     Trainable params\n",
      "0         Non-trainable params\n",
      "228 K     Total params\n",
      "0.914     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 207/207 [00:04<00:00, 47.67it/s, v_num=169, train_loss=824.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 207/207 [00:04<00:00, 47.50it/s, v_num=169, train_loss=824.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:13:39,136]\u001b[0m Trial 8 finished with value: 1513.13232421875 and parameters: {'latent_dim': 226, 'batch_size': 28}. Best is trial 8 with value: 1513.13232421875.\u001b[0m\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | fc1  | Linear | 27.1 K\n",
      "1 | fc2  | Linear | 107 K \n",
      "2 | fc3  | Linear | 56.3 K\n",
      "3 | fc4  | Linear | 24.7 K\n",
      "--------------------------------\n",
      "216 K     Trainable params\n",
      "0         Non-trainable params\n",
      "216 K     Total params\n",
      "0.864     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 142/142 [00:03<00:00, 45.83it/s, v_num=170, train_loss=154.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 142/142 [00:03<00:00, 45.61it/s, v_num=170, train_loss=154.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-04 16:14:14,975]\u001b[0m Trial 9 finished with value: 2267.47509765625 and parameters: {'latent_dim': 210, 'batch_size': 41}. Best is trial 8 with value: 1513.13232421875.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial - Loss: 1513.1323\n",
      "Best trial - Hyperparameters: {'batch_size': 28, 'latent_dim': 226}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction='minimize', storage='sqlite:///optuna.db')\n",
    "study.optimize(objective, n_trials=10)  # Adjust the number of trials as per your requirements\n",
    "\n",
    "# Print the best hyperparameters and objective value\n",
    "best_trial = study.best_trial\n",
    "print('Best trial - Loss: {:.4f}'.format(best_trial.value))\n",
    "print('Best trial - Hyperparameters:', best_trial.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on hypertunning:\n",
    "\n",
    "```\n",
    "[I 2023-05-04 15:04:50,382] A new study created in memory with name: no-name-c81c1788-d7b5-4191-95ee-9fb77d4a5160\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 32.4 K\n",
    "2 | fc3  | Linear | 18.7 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "102 K     Trainable params\n",
    "0         Non-trainable params\n",
    "102 K     Total params\n",
    "0.412     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 48.02it/s, v_num=3, train_loss=861.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 47.57it/s, v_num=3, train_loss=861.0]\n",
    "[I 2023-05-04 15:05:06,782] Trial 0 finished with value: 2392.20751953125 and parameters: {'latent_dim': 63}. Best is trial 0 with value: 2392.20751953125.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 31.4 K\n",
    "2 | fc3  | Linear | 18.2 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "101 K     Trainable params\n",
    "0         Non-trainable params\n",
    "101 K     Total params\n",
    "0.405     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 58.11it/s, v_num=4, train_loss=830.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.72it/s, v_num=4, train_loss=830.0]\n",
    "[I 2023-05-04 15:05:23,613] Trial 1 finished with value: 2351.134521484375 and parameters: {'latent_dim': 61}. Best is trial 1 with value: 2351.134521484375.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 108 K \n",
    "2 | fc3  | Linear | 56.6 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "216 K     Trainable params\n",
    "0         Non-trainable params\n",
    "216 K     Total params\n",
    "0.867     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 56.34it/s, v_num=5, train_loss=876.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 55.89it/s, v_num=5, train_loss=876.0]\n",
    "[I 2023-05-04 15:05:41,434] Trial 2 finished with value: 2495.365234375 and parameters: {'latent_dim': 211}. Best is trial 1 with value: 2351.134521484375.\n",
    "\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 113 K \n",
    "2 | fc3  | Linear | 59.1 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "224 K     Trainable params\n",
    "0         Non-trainable params\n",
    "224 K     Total params\n",
    "0.898     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.33it/s, v_num=6, train_loss=832.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 56.87it/s, v_num=6, train_loss=832.0]\n",
    "[I 2023-05-04 15:05:57,861] Trial 3 finished with value: 2569.151123046875 and parameters: {'latent_dim': 221}. Best is trial 1 with value: 2351.134521484375.\n",
    "\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 15.9 K\n",
    "2 | fc3  | Linear | 10.5 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "78.2 K    Trainable params\n",
    "0         Non-trainable params\n",
    "78.2 K    Total params\n",
    "0.313     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.40it/s, v_num=7, train_loss=761.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.09it/s, v_num=7, train_loss=761.0]\n",
    "[I 2023-05-04 15:06:15,607] Trial 4 finished with value: 2275.631591796875 and parameters: {'latent_dim': 31}. Best is trial 4 with value: 2275.631591796875.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 105 K \n",
    "2 | fc3  | Linear | 55.0 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "212 K     Trainable params\n",
    "0         Non-trainable params\n",
    "212 K     Total params\n",
    "0.849     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 58.07it/s, v_num=8, train_loss=907.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.63it/s, v_num=8, train_loss=907.0]\n",
    "[I 2023-05-04 15:06:32,468] Trial 5 finished with value: 2464.591796875 and parameters: {'latent_dim': 205}. Best is trial 4 with value: 2275.631591796875.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 24.7 K\n",
    "2 | fc3  | Linear | 14.8 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "91.3 K    Trainable params\n",
    "0         Non-trainable params\n",
    "91.3 K    Total params\n",
    "0.365     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 53.44it/s, v_num=9, train_loss=940.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 53.15it/s, v_num=9, train_loss=940.0]\n",
    "[I 2023-05-04 15:06:51,359] Trial 6 finished with value: 2352.005615234375 and parameters: {'latent_dim': 48}. Best is trial 4 with value: 2275.631591796875.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 23.1 K\n",
    "2 | fc3  | Linear | 14.1 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "89.0 K    Trainable params\n",
    "0         Non-trainable params\n",
    "89.0 K    Total params\n",
    "0.356     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.74it/s, v_num=10, train_loss=941.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 57.40it/s, v_num=10, train_loss=941.0]\n",
    "[I 2023-05-04 15:07:08,881] Trial 7 finished with value: 2336.918701171875 and parameters: {'latent_dim': 45}. Best is trial 4 with value: 2275.631591796875.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 28.3 K\n",
    "2 | fc3  | Linear | 16.6 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "96.7 K    Trainable params\n",
    "0         Non-trainable params\n",
    "96.7 K    Total params\n",
    "0.387     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 56.90it/s, v_num=11, train_loss=837.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 56.56it/s, v_num=11, train_loss=837.0]\n",
    "[I 2023-05-04 15:07:27,452] Trial 8 finished with value: 2333.72314453125 and parameters: {'latent_dim': 55}. Best is trial 4 with value: 2275.631591796875.\n",
    "GPU available: True (mps), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "  | Name | Type   | Params\n",
    "--------------------------------\n",
    "0 | fc1  | Linear | 27.1 K\n",
    "1 | fc2  | Linear | 115 K \n",
    "2 | fc3  | Linear | 59.9 K\n",
    "3 | fc4  | Linear | 24.7 K\n",
    "--------------------------------\n",
    "226 K     Trainable params\n",
    "0         Non-trainable params\n",
    "226 K     Total params\n",
    "0.907     Total estimated model params size (MB)\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 54.92it/s, v_num=12, train_loss=954.0]  \n",
    "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
    "Epoch 9: 100%|██████████| 91/91 [00:01<00:00, 54.46it/s, v_num=12, train_loss=954.0]\n",
    "[I 2023-05-04 15:07:45,962] Trial 9 finished with value: 2482.10107421875 and parameters: {'latent_dim': 224}. Best is trial 4 with value: 2275.631591796875.\n",
    "Best trial - Loss: 2275.6316\n",
    "Best trial - Hyperparameters: {'latent_dim': 31}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecvae-_b3Dgs6W-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
