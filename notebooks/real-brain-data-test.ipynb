{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing cVAE as harmonization tool on real brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:48.520715Z",
     "start_time": "2023-07-03T23:01:48.424168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:48.570179Z",
     "start_time": "2023-07-03T23:01:48.522046Z"
    }
   },
   "outputs": [],
   "source": [
    "from mecvae.lit import RealBrainMeasuresDataModule\n",
    "\n",
    "data_module = RealBrainMeasuresDataModule('/Users/ssilvari/Downloads/fedcombat_real_data/unified/non_harmonized_data.csv', batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:50.883627Z",
     "start_time": "2023-07-03T23:01:48.568308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/ssilvari/Downloads/fedcombat_real_data/unified/non_harmonized_data.csv...\n",
      "Number of numerical variables: self.n_num_cols=86\n",
      "Number of categorical variables: self.n_cat_cols=2\n",
      "Number of batches: 9\n"
     ]
    }
   ],
   "source": [
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:50.933298Z",
     "start_time": "2023-07-03T23:01:50.883497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              C(Sex)[Female]  C(Sex)[Male]  C(DX)[T.Autism]  C(DX)[T.CN]   \nPTID                                                                       \nA4_B10081264             1.0           0.0              0.0          1.0  \\\nA4_B10102783             1.0           0.0              0.0          1.0   \nA4_B10108368             1.0           0.0              0.0          1.0   \nA4_B10117310             0.0           1.0              0.0          1.0   \nA4_B10350512             0.0           1.0              0.0          1.0   \n...                      ...           ...              ...          ...   \nPPMI_85062               0.0           1.0              0.0          0.0   \nPPMI_85242               0.0           1.0              0.0          0.0   \nPPMI_90456               0.0           1.0              0.0          0.0   \nPPMI_91097               0.0           1.0              0.0          0.0   \nPPMI_92834               0.0           1.0              0.0          0.0   \n\n              C(DX)[T.GenCohort PD]  C(DX)[T.GenCohort Unaff]  C(DX)[T.MCI]   \nPTID                                                                          \nA4_B10081264                    0.0                       0.0           0.0  \\\nA4_B10102783                    0.0                       0.0           0.0   \nA4_B10108368                    0.0                       0.0           0.0   \nA4_B10117310                    0.0                       0.0           0.0   \nA4_B10350512                    0.0                       0.0           0.0   \n...                             ...                       ...           ...   \nPPMI_85062                      0.0                       0.0           0.0   \nPPMI_85242                      0.0                       0.0           0.0   \nPPMI_90456                      0.0                       0.0           0.0   \nPPMI_91097                      0.0                       0.0           0.0   \nPPMI_92834                      0.0                       0.0           0.0   \n\n              C(DX)[T.PD]  C(DX)[T.Prodromal]  C(DX)[T.SWEDD]  ...   \nPTID                                                           ...   \nA4_B10081264          0.0                 0.0             0.0  ...  \\\nA4_B10102783          0.0                 0.0             0.0  ...   \nA4_B10108368          0.0                 0.0             0.0  ...   \nA4_B10117310          0.0                 0.0             0.0  ...   \nA4_B10350512          0.0                 0.0             0.0  ...   \n...                   ...                 ...             ...  ...   \nPPMI_85062            0.0                 1.0             0.0  ...   \nPPMI_85242            0.0                 1.0             0.0  ...   \nPPMI_90456            0.0                 1.0             0.0  ...   \nPPMI_91097            0.0                 1.0             0.0  ...   \nPPMI_92834            0.0                 1.0             0.0  ...   \n\n              standardize(Q(\"rh_cuneus_thickness\"))   \nPTID                                                  \nA4_B10081264                              -0.625918  \\\nA4_B10102783                              -0.367826   \nA4_B10108368                              -0.126208   \nA4_B10117310                              -0.466670   \nA4_B10350512                              -0.021873   \n...                                             ...   \nPPMI_85062                                -2.218398   \nPPMI_85242                                -0.109734   \nPPMI_90456                                -0.181121   \nPPMI_91097                                 0.450379   \nPPMI_92834                                -0.691813   \n\n              standardize(Q(\"lh_parsopercularis_thickness\"))   \nPTID                                                           \nA4_B10081264                                        0.289726  \\\nA4_B10102783                                       -0.466325   \nA4_B10108368                                       -0.255220   \nA4_B10117310                                       -0.692158   \nA4_B10350512                                       -0.554695   \n...                                                      ...   \nPPMI_85062                                         -1.124188   \nPPMI_85242                                         -0.215945   \nPPMI_90456                                         -0.461416   \nPPMI_91097                                         -2.744297   \nPPMI_92834                                         -0.530148   \n\n              standardize(Q(\"rh_postcentral_thickness\"))   \nPTID                                                       \nA4_B10081264                                   -0.697299  \\\nA4_B10102783                                   -0.338380   \nA4_B10108368                                    0.416958   \nA4_B10117310                                    0.009826   \nA4_B10350512                                   -0.343737   \n...                                                  ...   \nPPMI_85062                                     -1.624062   \nPPMI_85242                                     -0.633015   \nPPMI_90456                                      0.240177   \nPPMI_91097                                      0.197321   \nPPMI_92834                                     -0.402664   \n\n              standardize(Q(\"lh_precuneus_thickness\"))   \nPTID                                                     \nA4_B10081264                                 -0.328659  \\\nA4_B10102783                                 -0.589114   \nA4_B10108368                                  0.863949   \nA4_B10117310                                 -0.200716   \nA4_B10350512                                 -0.246410   \n...                                                ...   \nPPMI_85062                                   -1.087176   \nPPMI_85242                                   -0.054496   \nPPMI_90456                                    0.164834   \nPPMI_91097                                   -2.206673   \nPPMI_92834                                   -0.461171   \n\n              standardize(Q(\"rh_caudalanteriorcingulate_thickness\"))   \nPTID                                                                   \nA4_B10081264                                           0.794624       \\\nA4_B10102783                                          -1.360042        \nA4_B10108368                                          -0.634536        \nA4_B10117310                                          -0.975436        \nA4_B10350512                                           0.510540        \n...                                                         ...        \nPPMI_85062                                            -0.647647        \nPPMI_85242                                            -0.949213        \nPPMI_90456                                             1.375903        \nPPMI_91097                                            -0.660759        \nPPMI_92834                                            -0.599572        \n\n              standardize(Q(\"rh_superiorparietal_thickness\"))   \nPTID                                                            \nA4_B10081264                                        -0.233408  \\\nA4_B10102783                                        -0.359543   \nA4_B10108368                                         0.422495   \nA4_B10117310                                        -0.601723   \nA4_B10350512                                         0.018862   \n...                                                       ...   \nPPMI_85062                                          -1.252580   \nPPMI_85242                                          -0.273771   \nPPMI_90456                                           1.386167   \nPPMI_91097                                          -1.459442   \nPPMI_92834                                          -0.132500   \n\n              standardize(Q(\"lh_parahippocampal_thickness\"))   \nPTID                                                           \nA4_B10081264                                        0.593521  \\\nA4_B10102783                                        1.105863   \nA4_B10108368                                        0.210886   \nA4_B10117310                                        0.807538   \nA4_B10350512                                       -1.569341   \n...                                                      ...   \nPPMI_85062                                          0.110363   \nPPMI_85242                                         -1.105639   \nPPMI_90456                                         -1.245074   \nPPMI_91097                                          0.090907   \nPPMI_92834                                         -1.099154   \n\n              standardize(Q(\"rh_superiortemporal_thickness\"))   \nPTID                                                            \nA4_B10081264                                         0.698243  \\\nA4_B10102783                                        -0.322887   \nA4_B10108368                                         0.366270   \nA4_B10117310                                        -0.398527   \nA4_B10350512                                        -0.339696   \n...                                                       ...   \nPPMI_85062                                          -0.385920   \nPPMI_85242                                          -0.268259   \nPPMI_90456                                          -0.726297   \nPPMI_91097                                          -1.701203   \nPPMI_92834                                          -0.074959   \n\n              standardize(Q(\"lh_paracentral_thickness\"))   \nPTID                                                       \nA4_B10081264                                    0.327895  \\\nA4_B10102783                                   -0.696486   \nA4_B10108368                                    0.866129   \nA4_B10117310                                   -0.783298   \nA4_B10350512                                   -0.609674   \n...                                                  ...   \nPPMI_85062                                     -3.062112   \nPPMI_85242                                     -0.366601   \nPPMI_90456                                      0.397344   \nPPMI_91097                                     -0.327535   \nPPMI_92834                                     -0.370941   \n\n              standardize(Q(\"lh_inferiorparietal_thickness\"))  \nPTID                                                           \nA4_B10081264                                        -0.624250  \nA4_B10102783                                        -0.095211  \nA4_B10108368                                        -0.462202  \nA4_B10117310                                        -0.895918  \nA4_B10350512                                        -0.228662  \n...                                                       ...  \nPPMI_85062                                          -0.814894  \nPPMI_85242                                          -0.381178  \nPPMI_90456                                           1.239301  \nPPMI_91097                                          -1.644198  \nPPMI_92834                                          -0.724338  \n\n[7231 rows x 96 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C(Sex)[Female]</th>\n      <th>C(Sex)[Male]</th>\n      <th>C(DX)[T.Autism]</th>\n      <th>C(DX)[T.CN]</th>\n      <th>C(DX)[T.GenCohort PD]</th>\n      <th>C(DX)[T.GenCohort Unaff]</th>\n      <th>C(DX)[T.MCI]</th>\n      <th>C(DX)[T.PD]</th>\n      <th>C(DX)[T.Prodromal]</th>\n      <th>C(DX)[T.SWEDD]</th>\n      <th>...</th>\n      <th>standardize(Q(\"rh_cuneus_thickness\"))</th>\n      <th>standardize(Q(\"lh_parsopercularis_thickness\"))</th>\n      <th>standardize(Q(\"rh_postcentral_thickness\"))</th>\n      <th>standardize(Q(\"lh_precuneus_thickness\"))</th>\n      <th>standardize(Q(\"rh_caudalanteriorcingulate_thickness\"))</th>\n      <th>standardize(Q(\"rh_superiorparietal_thickness\"))</th>\n      <th>standardize(Q(\"lh_parahippocampal_thickness\"))</th>\n      <th>standardize(Q(\"rh_superiortemporal_thickness\"))</th>\n      <th>standardize(Q(\"lh_paracentral_thickness\"))</th>\n      <th>standardize(Q(\"lh_inferiorparietal_thickness\"))</th>\n    </tr>\n    <tr>\n      <th>PTID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>A4_B10081264</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.625918</td>\n      <td>0.289726</td>\n      <td>-0.697299</td>\n      <td>-0.328659</td>\n      <td>0.794624</td>\n      <td>-0.233408</td>\n      <td>0.593521</td>\n      <td>0.698243</td>\n      <td>0.327895</td>\n      <td>-0.624250</td>\n    </tr>\n    <tr>\n      <th>A4_B10102783</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.367826</td>\n      <td>-0.466325</td>\n      <td>-0.338380</td>\n      <td>-0.589114</td>\n      <td>-1.360042</td>\n      <td>-0.359543</td>\n      <td>1.105863</td>\n      <td>-0.322887</td>\n      <td>-0.696486</td>\n      <td>-0.095211</td>\n    </tr>\n    <tr>\n      <th>A4_B10108368</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.126208</td>\n      <td>-0.255220</td>\n      <td>0.416958</td>\n      <td>0.863949</td>\n      <td>-0.634536</td>\n      <td>0.422495</td>\n      <td>0.210886</td>\n      <td>0.366270</td>\n      <td>0.866129</td>\n      <td>-0.462202</td>\n    </tr>\n    <tr>\n      <th>A4_B10117310</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.466670</td>\n      <td>-0.692158</td>\n      <td>0.009826</td>\n      <td>-0.200716</td>\n      <td>-0.975436</td>\n      <td>-0.601723</td>\n      <td>0.807538</td>\n      <td>-0.398527</td>\n      <td>-0.783298</td>\n      <td>-0.895918</td>\n    </tr>\n    <tr>\n      <th>A4_B10350512</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.021873</td>\n      <td>-0.554695</td>\n      <td>-0.343737</td>\n      <td>-0.246410</td>\n      <td>0.510540</td>\n      <td>0.018862</td>\n      <td>-1.569341</td>\n      <td>-0.339696</td>\n      <td>-0.609674</td>\n      <td>-0.228662</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>PPMI_85062</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-2.218398</td>\n      <td>-1.124188</td>\n      <td>-1.624062</td>\n      <td>-1.087176</td>\n      <td>-0.647647</td>\n      <td>-1.252580</td>\n      <td>0.110363</td>\n      <td>-0.385920</td>\n      <td>-3.062112</td>\n      <td>-0.814894</td>\n    </tr>\n    <tr>\n      <th>PPMI_85242</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.109734</td>\n      <td>-0.215945</td>\n      <td>-0.633015</td>\n      <td>-0.054496</td>\n      <td>-0.949213</td>\n      <td>-0.273771</td>\n      <td>-1.105639</td>\n      <td>-0.268259</td>\n      <td>-0.366601</td>\n      <td>-0.381178</td>\n    </tr>\n    <tr>\n      <th>PPMI_90456</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.181121</td>\n      <td>-0.461416</td>\n      <td>0.240177</td>\n      <td>0.164834</td>\n      <td>1.375903</td>\n      <td>1.386167</td>\n      <td>-1.245074</td>\n      <td>-0.726297</td>\n      <td>0.397344</td>\n      <td>1.239301</td>\n    </tr>\n    <tr>\n      <th>PPMI_91097</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.450379</td>\n      <td>-2.744297</td>\n      <td>0.197321</td>\n      <td>-2.206673</td>\n      <td>-0.660759</td>\n      <td>-1.459442</td>\n      <td>0.090907</td>\n      <td>-1.701203</td>\n      <td>-0.327535</td>\n      <td>-1.644198</td>\n    </tr>\n    <tr>\n      <th>PPMI_92834</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.691813</td>\n      <td>-0.530148</td>\n      <td>-0.402664</td>\n      <td>-0.461171</td>\n      <td>-0.599572</td>\n      <td>-0.132500</td>\n      <td>-1.099154</td>\n      <td>-0.074959</td>\n      <td>-0.370941</td>\n      <td>-0.724338</td>\n    </tr>\n  </tbody>\n</table>\n<p>7231 rows × 96 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 96]) torch.Size([6, 9])\n"
     ]
    }
   ],
   "source": [
    "from mecvae.lit_models.cvae import LitFlexCVAE\n",
    "\n",
    "# Import LitCVAE\n",
    "\n",
    "model = LitFlexCVAE(data_dim=data_module.n_features,\n",
    "                    conditioning_dim=data_module.n_batches,\n",
    "                    # lr=1e-5, activation=nn.Tanh(),\n",
    "                    hidden_dim=[256, 128],\n",
    "                    z_dim=64,\n",
    "                    optimizer='adam')\n",
    "# Test model with batch\n",
    "x, y = next(iter(data_module.train_dataloader()))\n",
    "print(x.shape, y.shape)\n",
    "x_hat, mu, log_var = model(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:51.046885Z",
     "start_time": "2023-07-03T23:01:50.933685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:56.578108Z",
     "start_time": "2023-07-03T23:01:51.023598Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:517: UserWarning: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/ssilvari/Downloads/fedcombat_real_data/unified/non_harmonized_data.csv...\n",
      "Number of numerical variables: self.n_num_cols=86\n",
      "Number of categorical variables: self.n_cat_cols=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | encoder   | Sequential | 60.0 K\n",
      "1 | fc_mean   | Linear     | 8.3 K \n",
      "2 | fc_logvar | Linear     | 8.3 K \n",
      "3 | decoder   | Sequential | 67.2 K\n",
      "-----------------------------------------\n",
      "143 K     Trainable params\n",
      "0         Non-trainable params\n",
      "143 K     Total params\n",
      "0.575     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# Train model using pytorch lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Define logger\n",
    "logger = TensorBoardLogger(save_dir='/Users/ssilvari/PycharmProjects/Fed-MECVAE/lightning_logs', name='cVAE (Flex)')\n",
    "\n",
    "# Save best model using a model checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Validation/loss',\n",
    "    dirpath=logger.log_dir,\n",
    "    filename='cVAE (Flex)-{epoch:02d}-{Validation-loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# Define early stopping callback\n",
    "callbacks = [EarlyStopping(monitor='Validation/loss', patience=10), checkpoint_callback]\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(max_epochs=2000, callbacks=callbacks, logger=logger, enable_progress_bar=False, accelerator='cpu',\n",
    "                  gradient_clip_val=1.0, accumulate_grad_batches=4, precision=16)\n",
    "# Continue from checkpoint\n",
    "ckpt_path = checkpoint_callback.best_model_path if os.path.exists(checkpoint_callback.best_model_path) else None\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model, data_module, ckpt_path=ckpt_path)\n",
    "\n",
    "# Load best model\n",
    "model = LitFlexCVAE.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:56.619560Z",
     "start_time": "2023-07-03T23:01:56.578431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/Users/ssilvari/PycharmProjects/Fed-MECVAE/lightning_logs/cVAE (Flex)/version_4/cVAE (Flex)-epoch=01-Validation-loss=0.00.ckpt'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T23:01:57.815317Z",
     "start_time": "2023-07-03T23:01:56.621275Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 22\u001B[0m\n\u001B[1;32m     18\u001B[0m y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mone_hot(y, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(y_probs)) \u001B[38;5;66;03m#* 0\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# print(x.shape, y.shape)\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Predict x\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m x_hat, mu, log_var \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Append to list\u001B[39;00m\n\u001B[1;32m     25\u001B[0m x_hats\u001B[38;5;241m.\u001B[39mappend(x_hat\u001B[38;5;241m.\u001B[39mdetach())\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/Fed-MECVAE/mecvae/lit_models/cvae.py:66\u001B[0m, in \u001B[0;36mLitFlexCVAE.forward\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     63\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([x, y], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     65\u001B[0m \u001B[38;5;66;03m# Encode the data\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Compute the latent space\u001B[39;00m\n\u001B[1;32m     69\u001B[0m mu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_mean(x)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Predict the whole dataset removing the batch effect by sampling y from a categorical distribution\n",
    "import torch\n",
    "\n",
    "# Compute probabilities of y\n",
    "y_probs = data_module.y.value_counts(normalize=True).sort_index().values\n",
    "y_probs = torch.tensor(y_probs, dtype=torch.float32)\n",
    "\n",
    "# Extract tensors from data_module\n",
    "x = torch.tensor(data_module.X.values, dtype=torch.float32)\n",
    "\n",
    "# Create a list of predictions (we'll sample 100 times)\n",
    "x_hats = []\n",
    "for _ in range(100):\n",
    "    with torch.no_grad():\n",
    "        # Sample y\n",
    "        y = torch.multinomial(y_probs, len(data_module.y), replacement=True)\n",
    "        # One hot encode\n",
    "        y = torch.nn.functional.one_hot(y, num_classes=len(y_probs)) #* 0\n",
    "        # print(x.shape, y.shape)\n",
    "\n",
    "        # Predict x\n",
    "        x_hat, mu, log_var = model(x, y)\n",
    "\n",
    "        # Append to list\n",
    "        x_hats.append(x_hat.detach())\n",
    "\n",
    "# Compute x_hat mean from samples\n",
    "x_hat_mean = torch.stack(x_hats).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predictions\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "x_hat_df = pd.DataFrame(x_hat_mean.numpy(), columns=data_module.X.columns, index=data_module.df.index)\n",
    "\n",
    "# Remove covariate columns. Those containing 'Age', 'Sex', 'DX'\n",
    "x_hat_df = x_hat_df.loc[:, ~x_hat_df.columns.str.contains('Age|Sex|DX')]\n",
    "\n",
    "# Extract the phenotype name using a regex knwong that the column names are wrapped by something like this: standardize(Q(\"lh_inferiorparietal_thickness\")) where lh_inferiorparietal_thickness is the phenotype name\n",
    "x_hat_df.columns = [re.search(r'Q\\(\"(.*)\"\\)', col).group(1) for col in x_hat_df.columns]\n",
    "\n",
    "# Back transform from the standardization\n",
    "x_hat_df_destd = x_hat_df * data_module.df[x_hat_df.columns].std() + data_module.df[x_hat_df.columns].mean()\n",
    "print(x_hat_df_destd.head())\n",
    "\n",
    "# Join the rest of the columns present in data_module.df that are not in x_hat_df_destd\n",
    "x_hat_df_destd = x_hat_df_destd.join(data_module.df.loc[:, ~data_module.df.columns.isin(x_hat_df_destd.columns)])\n",
    "print(x_hat_df_destd)\n",
    "\n",
    "# save as csv\n",
    "root_dir = os.path.dirname(data_module.csv_file)\n",
    "harmonized_csv = root_dir + os.sep + 'harmonized_cVAE.csv'\n",
    "x_hat_df_destd.to_csv(harmonized_csv)\n",
    "print(f'Harmonized data saved to {harmonized_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to `methods_params.csv` in the same folder\n",
    "methods_params_file = root_dir + os.sep + 'methods_params.csv'\n",
    "methods_params = pd.read_csv(methods_params_file)\n",
    "\n",
    "# Create new entry\n",
    "cvae_mparams = pd.DataFrame([{\n",
    "    'Method': 'cVAE',\n",
    "    'data_file_path': harmonized_csv,\n",
    "    'classification_results_path': root_dir + os.sep + 'benchmark cVAE.csv'\n",
    "}])\n",
    "\n",
    "methods_params = pd.concat([methods_params, cvae_mparams], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# save as csv ignore index\n",
    "methods_params.to_csv(methods_params_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notify that the process is done\n",
    "import os\n",
    "os.system('afplay /System/Library/Sounds/Funk.aiff')\n",
    "\n",
    "# Print date time when finished\n",
    "import datetime\n",
    "print(f'Finished at {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
