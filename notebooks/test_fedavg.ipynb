{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_data_loaders(n_clients, X, y, batch_size=32, shuffle=True):\n",
    "    # Split the data into n_clients subsets\n",
    "    X_split = torch.chunk(X, n_clients)\n",
    "    y_split = torch.chunk(y, n_clients)\n",
    "\n",
    "    # Create a data loader for each client\n",
    "    data_loaders = []\n",
    "    for i in range(n_clients):\n",
    "        dataset = TensorDataset(X_split[i], y_split[i])\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_regression\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mecvae.federated import FederatedTrainer\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# Define the linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(X))\n",
    "X_train, X_test = X_tensor[:train_size], X_tensor[train_size:]\n",
    "y_train, y_test = y_tensor[:train_size], y_tensor[train_size:]\n",
    "\n",
    "# Generate data loaders for n_clients\n",
    "n_clients = 5\n",
    "train_loaders = generate_data_loaders(n_clients, X_train, y_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssilvari/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m federated_trainer \u001b[39m=\u001b[39m FederatedTrainer(model, criterion, optimizer_fn, learning_rate, n_clients, patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Train the model using federated learning\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m federated_trainer\u001b[39m.\u001b[39;49mtrain(train_loaders, X_test, grad_steps\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/PycharmProjects/Fed-MECVAE/mecvae/federated.py:88\u001b[0m, in \u001b[0;36mFederatedTrainer.train\u001b[0;34m(self, train_loaders, test_loader, grad_steps)\u001b[0m\n\u001b[1;32m     85\u001b[0m epoch \u001b[39m=\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m grad_steps\n\u001b[1;32m     87\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_client_model(client_model, client_optimizer, train_loader, grad_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluate(client_model, test_loader)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_losses\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mClient\u001b[39m\u001b[39m'\u001b[39m: client_model, \u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[1;32m     91\u001b[0m                                               \u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m: train_loss}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_losses\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mClient\u001b[39m\u001b[39m'\u001b[39m: client_model, \u001b[39m'\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[1;32m     93\u001b[0m                                             \u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m: test_loss}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/PycharmProjects/Fed-MECVAE/mecvae/federated.py:53\u001b[0m, in \u001b[0;36mFederatedTrainer._evaluate\u001b[0;34m(self, model, data_loader)\u001b[0m\n\u001b[1;32m     50\u001b[0m steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     54\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     55\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, labels)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Initialize the linear regression model, criterion, optimizer, and learning rate\n",
    "model = LinearRegression()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_fn = optim.SGD\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize the federated trainer\n",
    "federated_trainer = FederatedTrainer(model, criterion, optimizer_fn, learning_rate, n_clients, patience=3)\n",
    "\n",
    "# Train the model using federated learning\n",
    "federated_trainer.train(train_loaders, [X_test, y_test], grad_steps='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the aggregated model after federated averaging\n",
    "aggregated_model = federated_trainer.get_aggregated_model()\n",
    "\n",
    "# Test the aggregated model\n",
    "aggregated_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = aggregated_model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred, y_test_tensor)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Plot train losses\n",
    "train_losses = federated_trainer.get_train_losses()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=train_losses, x='Epoch', y='Loss', hue='Client')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Losses')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot test losses\n",
    "test_losses = federated_trainer.get_test_losses()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=test_losses, x='Epoch', y='Loss', hue='Client')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Losses')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "from copy import deepcopy\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, dataset, model, loss_fn, num_epochs, client_id, lr=1e-2):\n",
    "        self.dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        self.model = model\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_epochs = num_epochs\n",
    "        self.writer = SummaryWriter(f'runs/Client_{client_id}')\n",
    "        self.iteration = 0\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in self.dataloader:\n",
    "            pred = self.model(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            self.writer.add_scalar('Loss/train', loss.item(), self.iteration)\n",
    "            self.iteration += 1\n",
    "\n",
    "        return self.model.state_dict(), len(self.dataloader)\n",
    "\n",
    "class Server:\n",
    "    clients = None\n",
    "\n",
    "    def __init__(self, model, dataset, loss_fn, lr=1e-3, validation_split=0.2, early_stopping=True, patience=5):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.writer = SummaryWriter('runs/Server')\n",
    "\n",
    "        num_validation = int(len(dataset) * validation_split)\n",
    "        num_train = len(dataset) - num_validation\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [num_train, num_validation])\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0  # for early stopping\n",
    "\n",
    "    def create_clients(self, num_clients):\n",
    "        self.clients = [Client(self.train_dataset, deepcopy(self.model), self.loss_fn, num_epochs=1, client_id=i+1, lr=self.lr) for i in range(num_clients)]\n",
    "\n",
    "    def federated_learning(self, num_rounds):\n",
    "        with tqdm(range(num_rounds), desc='Rounds') as pbar:\n",
    "            for round in pbar:\n",
    "                global_weights = []\n",
    "                global_batches = 0\n",
    "                for client in self.clients:\n",
    "                    client_weights, num_batches = client.train()\n",
    "                    global_weights.append((client_weights, num_batches))\n",
    "                    global_batches += num_batches\n",
    "\n",
    "                # federated averaging\n",
    "                new_state_dict = {}\n",
    "                for key in self.model.state_dict().keys():\n",
    "                    global_sum = sum([client_weights[key] * num_batches for client_weights, num_batches in global_weights])\n",
    "                    new_state_dict[key] = global_sum / global_batches\n",
    "                self.model.load_state_dict(new_state_dict)\n",
    "\n",
    "                # validate the model\n",
    "                val_loss = self.validate()\n",
    "\n",
    "                # Update progress bar description\n",
    "                pbar.set_description(f\"Round: {round+1}, Validation Loss: {val_loss:.4f}\")\n",
    "                pbar.update()\n",
    "\n",
    "                self.writer.add_scalar('Loss/validation', val_loss, round)\n",
    "\n",
    "                # update client models\n",
    "                for client in self.clients:\n",
    "                    client.model.load_state_dict(new_state_dict)\n",
    "\n",
    "                # early stopping\n",
    "                if self.early_stopping:\n",
    "                    if val_loss < self.best_val_loss:\n",
    "                        self.best_val_loss = val_loss\n",
    "                        self.wait = 0\n",
    "                    else:\n",
    "                        self.wait += 1\n",
    "                    if self.wait >= self.patience:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=32, shuffle=False)\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                pred = self.model(X)\n",
    "                loss = self.loss_fn(pred, y)\n",
    "                total_loss += loss.item() * len(X)\n",
    "        return total_loss / len(self.val_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf runs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12000, 12000, 12000, 12000, 12000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Round: 100, Validation Loss: 0.2066: 100%|██████████| 100/100 [07:12<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "# # Define the CNN architecture\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.fc1 = nn.Linear(7*7*64, 128)\n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = nn.functional.relu(x)\n",
    "#         x = nn.functional.max_pool2d(x, 2)\n",
    "#         x = self.conv2(x)\n",
    "#         x = nn.functional.relu(x)\n",
    "#         x = nn.functional.max_pool2d(x, 2)\n",
    "#         x = x.view(x.size(0), -1) # Flatten layer\n",
    "#         x = self.fc1(x)\n",
    "#         x = nn.functional.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# Define the model class\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)  # Input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(128, 10)   # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation to hidden layer\n",
    "        x = self.fc2(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # Normalizing with MNIST mean and std\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Initialize the server\n",
    "model = SimpleClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "server = Server(model, dataset, loss_fn, lr=1e-3)\n",
    "\n",
    "# Create the clients\n",
    "num_clients = 5\n",
    "num_samples_per_client = len(dataset) // num_clients\n",
    "client_datasets = random_split(dataset, [num_samples_per_client]*num_clients)\n",
    "print([len(cd) for cd in client_datasets])\n",
    "server.create_clients(num_clients)\n",
    "\n",
    "for i, client in enumerate(server.clients):\n",
    "    client.dataloader = DataLoader(client_datasets[i], batch_size=32, shuffle=True)\n",
    "\n",
    "# Run federated learning\n",
    "server.federated_learning(num_rounds=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5492603778839111\n",
      "Epoch [2/10], Loss: 0.3300962448120117\n",
      "Epoch [3/10], Loss: 0.3385804295539856\n",
      "Epoch [4/10], Loss: 0.1776585876941681\n",
      "Epoch [5/10], Loss: 0.4287305176258087\n",
      "Epoch [6/10], Loss: 0.19159863889217377\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     36\u001b[0m \u001b[39m# Print the training loss for each epoch\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/optim/sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[0;32m---> 76\u001b[0m sgd(params_with_grad,\n\u001b[1;32m     77\u001b[0m     d_p_list,\n\u001b[1;32m     78\u001b[0m     momentum_buffer_list,\n\u001b[1;32m     79\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     80\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     81\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     83\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     85\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m     86\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     88\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/optim/sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[0;32m--> 222\u001b[0m func(params,\n\u001b[1;32m    223\u001b[0m      d_p_list,\n\u001b[1;32m    224\u001b[0m      momentum_buffer_list,\n\u001b[1;32m    225\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    226\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[1;32m    229\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[1;32m    230\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[1;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/mecvae-_b3Dgs6W-py3.11/lib/python3.11/site-packages/torch/optim/sgd.py:265\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         d_p \u001b[39m=\u001b[39m buf\n\u001b[0;32m--> 265\u001b[0m param\u001b[39m.\u001b[39;49madd_(d_p, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleClassifier()\n",
    "\n",
    "# Define the loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the training loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecvae-_b3Dgs6W-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
